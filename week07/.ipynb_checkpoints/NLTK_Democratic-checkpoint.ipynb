{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>TF (term frequency) : number of time a word occurs in a document</li>\n",
    "<li>IDF (inverse document frequency) : log fraction of the total number of documents by the number of documents containing the term </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>We first split all the congress members into two groups : democratics and republicans </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "congress1 = pd.read_csv('../data/H113.csv')\n",
    "congress2 = pd.read_csv('../data/H114.csv')\n",
    "congress3 = pd.read_csv('../data/H115.csv')\n",
    "\n",
    "congress_members = pd.concat([congress1,congress2, congress3])\n",
    "congress_members = congress_members.drop_duplicates()\n",
    "\n",
    "republicans, democratics = [], []\n",
    "\n",
    "for congressman in congress1.WikiPageName.sort_values():\n",
    "    party = congress1[congress1.WikiPageName == congressman].Party.values\n",
    "    if (str(party[0])) == \"Republican\":\n",
    "        republicans.append(congressman)\n",
    "    else:\n",
    "        democratics.append(congressman)\n",
    "   \n",
    "republicans_new, democratics_new = [], []\n",
    "for elt in republicans:\n",
    "    for el in elt.split(\"_\"):\n",
    "        republicans_new.append(el.replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "        \n",
    "for elt in democratics:\n",
    "    for el in elt.split(\"_\"):\n",
    "        democratics_new.append(el.replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "\n",
    "republicans_new = list(set(republicans_new))# list of all names and surnames of politicians\n",
    "democratics_new = list(set(democratics_new))\n",
    "\n",
    "congress_men = set(republicans_new + democratics_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "baseurl = \"http://en.wikipedia.org/w/api.php/?\"\n",
    "action = \"action=query\"\n",
    "content = \"prop=extracts&exlimit=max&explaintext\"\n",
    "rvprop =\"rvprop=timestamp|content\"\n",
    "dataformat = \"format=json\"\n",
    "rvdir = \"rvdir=older\" #sort revisions from newest to oldest\n",
    "start = \"rvend=2017-01-03T00:00:00Z\" #start of my time period\n",
    "end = \"rvstart=2018-09-25T00:00:00Z\" #end of my time period\n",
    "limit = \"rvlimit=1\" #consider only the first revision\n",
    "\n",
    "file = open(\"../data/republicans.txt\",\"w\")\n",
    "republicans_words_list = []\n",
    "for member in republicans:\n",
    "    title = \"titles=\" + member\n",
    "    query = \"%s%s&%s&%s&%s&%s&%s&%s&%s&%s\" % (baseurl, action, title, content, rvprop, dataformat, rvdir, end, start, limit)\n",
    "    text = requests.get(query).text\n",
    "    text = str(json.loads(text)[\"query\"][\"pages\"])\n",
    "    text = text.strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [w for w in tokens if re.search(r'\\w',w)]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    tokens = [word for word in tokens if not re.search(r'[0-9]+$',word)]\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', w) for w in tokens]\n",
    "    \n",
    "    tokens = [w.lower() for w in tokens if w not in congress_men]\n",
    "    file.write(\" \".join(tokens))\n",
    "    print(republicans.index(member))\n",
    "\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-344b4440e933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"titles=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s%s&%s&%s&%s&%s&%s&%s&%s&%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbaseurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "baseurl = \"http://en.wikipedia.org/w/api.php/?\"\n",
    "action = \"action=query\"\n",
    "content = \"prop=extracts&exlimit=max&explaintext\"\n",
    "rvprop =\"rvprop=timestamp|content\"\n",
    "dataformat = \"format=json\"\n",
    "rvdir = \"rvdir=older\" #sort revisions from newest to oldest\n",
    "start = \"rvend=2017-01-03T00:00:00Z\" #start of my time period\n",
    "end = \"rvstart=2018-09-25T00:00:00Z\" #end of my time period\n",
    "limit = \"rvlimit=1\" #consider only the first revision\n",
    "\n",
    "file = open(\"../data/democratics.txt\",\"w\")\n",
    "democratics_words_list = []\n",
    "for member in democratics:\n",
    "    title = \"titles=\" + member\n",
    "    query = \"%s%s&%s&%s&%s&%s&%s&%s&%s&%s\" % (baseurl, action, title, content, rvprop, dataformat, rvdir, end, start, limit)\n",
    "    text = requests.get(query).text\n",
    "    text = str(json.loads(text)[\"query\"][\"pages\"])\n",
    "    text = text.strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [w for w in tokens if re.search(r'\\w',w)]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    tokens = [word for word in tokens if not re.search(r'[0-9]+$',word)]\n",
    "    tokens = [re.sub('[^A-Za-z0-9]+', '', w) for w in tokens]\n",
    "    \n",
    "    tokens = [w.lower() for w in tokens if w not in congress_men]\n",
    "    file.write(\" \".join(tokens))\n",
    "    print(democratics.index(member))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "dem = open(\"../data/democratics.txt\",\"r\").read()\n",
    "dem_words = word_tokenize(dem)\n",
    "dem_text = nltk.Text(dem_words)\n",
    "\n",
    "fd_dem = FreqDist(dem_text)\n",
    "print(fd_dem.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFs are not necessarily a good description because it can give importance to meaningless words like \"the\". \n",
    "That is why IDF is a good tool because it gives less importance to word that occurs too many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "for elt in (fd_dem.most_common()[-5:]):\n",
    "    print(elt[0] + \" : \" + str(log( len(dem_words) / int(elt[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud().generate(dem)\n",
    "plt.imshow(wordcloud, interpolation='bilinear');\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Modularity :</u> it mesures the strength of a network into modules. Networks with light modularity have dense connections between the nodes within modules but sparse between nodes of different modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "import io\n",
    "\n",
    "congress1 = pd.read_csv('../data/H115.csv')\n",
    "path_folder = \"../data/congress115/congress115\"\n",
    "\n",
    "congress113_graph = nx.Graph()\n",
    "for congressman in congress1.WikiPageName.sort_values():\n",
    "    #new node with congressman's info\n",
    "    congress113_graph.add_node(congressman,\n",
    "                           state=congress1[congress1.WikiPageName == congressman].State,\n",
    "                           party=congress1[congress1.WikiPageName == congressman].Party)\n",
    "    \n",
    "    f = io.open(path_folder+congressman+'.txt', 'r', encoding='utf-8').read()\n",
    "    links = re.findall(\"\\[\\[(.*?)\\]\\]\", f)\n",
    "    for l in links:\n",
    "        match = re.search(r'([a-zA-Z0-9_\\s\\(\\)\\-\\,.]*)\\|([a-zA-Z0-9_\\s\\(\\)\\-\\,.]*)', l)\n",
    "        if match == None:\n",
    "            m = l.replace(\" \", \"_\")\n",
    "            if m in set(congress1.WikiPageName):\n",
    "                congress113_graph.add_edge(congressman, m)\n",
    "        else:\n",
    "            m1 = match.group(1).replace(\" \", \"_\")\n",
    "            m2 = match.group(2).replace(\" \", \"_\")\n",
    "            if m1 in set(congress1.WikiPageName):\n",
    "                congress113_graph.add_edge(congressman, m1)\n",
    "            if m2 in set(congress1.WikiPageName):\n",
    "                congress113_graph.add_edge(congressman, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(congress113_graph.edges()) #total number of edges\n",
    "\n",
    "list_attributes = nx.get_node_attributes(congress113_graph, 'party')\n",
    "\n",
    "L_rep, L_dem = 0, 0 \n",
    "for edge in congress113_graph.edges():\n",
    "    if congress1[congress1.WikiPageName == edge[0]].Party.values[0] == congress1[congress1.WikiPageName == edge[1]].Party.values[0] == 'Republican':\n",
    "        L_rep += 1\n",
    "    elif congress1[congress1.WikiPageName == edge[0]].Party.values[0] == congress1[congress1.WikiPageName == edge[1]].Party.values[0] == 'Democratic':\n",
    "        L_dem += 1\n",
    "    #     print(list_attributes[edge[0]])\n",
    "\n",
    "K_rep, K_dem = 0, 0\n",
    "for elt in list(congress113_graph.degree()):\n",
    "    if congress1[congress1.WikiPageName == elt[0]].Party.values[0] == 'Republican':\n",
    "        K_rep += elt[1]\n",
    "    else:\n",
    "        K_dem += elt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity = (L_rep/L - pow(K_rep/(2*L),2)) + (L_dem/L - pow(K_dem/(2*L),2))\n",
    "print(modularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modularity is quiet low which means that nodes from different modules are well connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Louvain communities detection\n",
    "import community\n",
    "\n",
    "partition = community.best_partition(congress113_graph)\n",
    "\n",
    "#drawing\n",
    "size = float(len(set(partition.values())))\n",
    "pos = nx.spring_layout(congress113_graph)\n",
    "count = 0.\n",
    "for com in set(partition.values()) :\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "    nx.draw_networkx_nodes(congress113_graph, pos, list_nodes, node_size = 20,\n",
    "                                node_color = str(count / size))\n",
    "\n",
    "\n",
    "nx.draw_networkx_edges(congress113_graph,pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
